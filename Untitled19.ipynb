{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Ouhammou1/project01.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtLq7sjeW-lx",
        "outputId": "6b8fecb3-6852-46fb-a990-682614a84dc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project01'...\n",
            "remote: Enumerating objects: 14510, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 14510 (delta 0), reused 0 (delta 0), pack-reused 14506 (from 4)\u001b[K\n",
            "Receiving objects: 100% (14510/14510), 1.31 GiB | 34.39 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Updating files: 100% (14422/14422), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "r8DzGro2WyVj",
        "outputId": "f5edfdf0-d7d8-435e-84ff-975e4ed9e50d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14380 files belonging to 30 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,311,744\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │        \u001b[38;5;34m15,390\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,744</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,390</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,907,649\u001b[0m (22.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,907,649</span> (22.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,855,006\u001b[0m (7.08 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,855,006</span> (7.08 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,052,643\u001b[0m (15.46 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,052,643</span> (15.46 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Phase 1: Training classifier head ===\n",
            "Epoch 1/20\n",
            "\u001b[1m134/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3:06\u001b[0m 8s/step - accuracy: 0.0287 - loss: 4.3591 - top3_accuracy: 0.1035"
          ]
        }
      ],
      "source": [
        "# Medicinal Plants Identification - Complete CNN Solution\n",
        "# Import all required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks, applications\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Constants and Configuration\n",
        "DATASET_PATH = \"/content/project01/data\"\n",
        "IMAGE_SIZE = (256, 256)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "INITIAL_LR = 0.0001\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "CLASS_NAMES = [\n",
        "    'Drumstick', 'Mint', 'Curry', 'Hibiscus', 'Jamaica_Cherry-Gasagase',\n",
        "    'Parijata', 'Pomegranate', 'Mexican_Mint', 'Oleander', 'Tulsi',\n",
        "    'Jackfruit', 'Lemon', 'Indian_Beech', 'Sandalwood', 'Basale',\n",
        "    'Crape_Jasmine', 'Indian_Mustard', 'Arive-Dantu', 'Roxburgh_fig',\n",
        "    'Jamun', 'Guava', 'Mango', 'Neem', 'Jasmine', 'Rose_apple',\n",
        "    'Peepal', 'Karanda', 'Fenugreek', 'Betel', 'Rasna'\n",
        "]\n",
        "\n",
        "# Data Loading and Preparation\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"Load and split dataset with proper preprocessing\"\"\"\n",
        "    # Load full dataset\n",
        "    full_dataset = image_dataset_from_directory(\n",
        "        DATASET_PATH,\n",
        "        shuffle=True,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        label_mode='categorical',\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Calculate split sizes\n",
        "    dataset_size = len(full_dataset) * BATCH_SIZE\n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    val_size = int(0.15 * len(full_dataset))\n",
        "\n",
        "    # Split dataset\n",
        "    train_ds = full_dataset.take(train_size)\n",
        "    remaining = full_dataset.skip(train_size)\n",
        "    val_ds = remaining.take(val_size)\n",
        "    test_ds = remaining.skip(val_size)\n",
        "\n",
        "    # Optimize dataset performance\n",
        "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "# Data Augmentation\n",
        "def create_augmentation_layer():\n",
        "    \"\"\"Create data augmentation pipeline\"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        layers.RandomRotation(0.25),\n",
        "        layers.RandomZoom(0.25),\n",
        "        layers.RandomContrast(0.2),\n",
        "        layers.RandomBrightness(0.2),\n",
        "        layers.GaussianNoise(0.01)\n",
        "    ], name=\"data_augmentation\")\n",
        "\n",
        "# Model Architecture\n",
        "def build_model():\n",
        "    \"\"\"Build and compile the CNN model\"\"\"\n",
        "    # Input and augmentation\n",
        "    inputs = layers.Input(shape=IMAGE_SIZE + (3,))\n",
        "    x = create_augmentation_layer()(inputs)\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "\n",
        "    # Base model (EfficientNetB0)\n",
        "    base_model = applications.EfficientNetB0(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=IMAGE_SIZE + (3,),\n",
        "        pooling='avg'\n",
        "    )\n",
        "    base_model.trainable = False  # Freeze initially\n",
        "\n",
        "    # Feature extraction\n",
        "    x = base_model(x, training=False)\n",
        "\n",
        "    # Classification head\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(len(CLASS_NAMES), activation='softmax')(x)\n",
        "\n",
        "    # Compile model\n",
        "    model = models.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=INITIAL_LR),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_accuracy')]\n",
        "    )\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "# Callbacks\n",
        "def create_callbacks():\n",
        "    \"\"\"Create training callbacks\"\"\"\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    return [\n",
        "        callbacks.ModelCheckpoint(\n",
        "            'best_model.keras',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.TensorBoard(\n",
        "            log_dir=log_dir,\n",
        "            histogram_freq=1\n",
        "        ),\n",
        "        callbacks.CSVLogger('training_log.csv')\n",
        "    ]\n",
        "\n",
        "# Training Process\n",
        "def train_model(model, train_ds, val_ds, base_model):\n",
        "    \"\"\"Execute training process with two phases\"\"\"\n",
        "    # Phase 1: Train only the classifier head\n",
        "    print(\"\\n=== Phase 1: Training classifier head ===\")\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=20,\n",
        "        callbacks=create_callbacks()\n",
        "    )\n",
        "\n",
        "    # Phase 2: Fine-tune top layers of base model\n",
        "    print(\"\\n=== Phase 2: Fine-tuning top layers ===\")\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-20]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Recompile with lower learning rate\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=INITIAL_LR/10),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_accuracy')]\n",
        "    )\n",
        "\n",
        "    # Continue training\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        initial_epoch=history.epoch[-1] + 1,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=create_callbacks()\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Visualization\n",
        "def plot_results(history):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.plot(history.history['top3_accuracy'], label='Train Top-3 Accuracy')\n",
        "    plt.plot(history.history['val_top3_accuracy'], label='Validation Top-3 Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Evaluation and Testing\n",
        "def evaluate_model(model, test_ds):\n",
        "    \"\"\"Evaluate model on test set and show metrics\"\"\"\n",
        "    print(\"\\n=== Final Evaluation ===\")\n",
        "    results = model.evaluate(test_ds)\n",
        "    print(f\"Test Loss: {results[0]:.4f}\")\n",
        "    print(f\"Test Accuracy: {results[1]:.4f}\")\n",
        "    print(f\"Test Top-3 Accuracy: {results[2]:.4f}\")\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    for images, labels in test_ds:\n",
        "        y_pred.extend(model.predict(images))\n",
        "        y_true.extend(labels.numpy())\n",
        "\n",
        "    # Convert to class indices\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_true, axis=1)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true_classes, y_pred_classes, target_names=CLASS_NAMES))\n",
        "\n",
        "    # Confusion matrix\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "# Prediction Utility\n",
        "def predict_plant(image_path, model):\n",
        "    \"\"\"Make prediction on a single image\"\"\"\n",
        "    img = tf.keras.utils.load_img(image_path, target_size=IMAGE_SIZE)\n",
        "    img_array = tf.keras.utils.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0) / 255.0\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "    top_k_values, top_k_indices = tf.math.top_k(score, k=3)\n",
        "\n",
        "    print(f\"\\nPrediction for {os.path.basename(image_path)}:\")\n",
        "    for i in range(3):\n",
        "        print(f\"{i+1}. {CLASS_NAMES[top_k_indices.numpy()[i]]}: \"\n",
        "              f\"{100 * top_k_values.numpy()[i]:.2f}%\")\n",
        "\n",
        "# Main Execution\n",
        "def main():\n",
        "    # Load data\n",
        "    train_ds, val_ds, test_ds = load_and_prepare_data()\n",
        "\n",
        "    # Build model\n",
        "    model, base_model = build_model()\n",
        "    model.summary()\n",
        "\n",
        "    # Train model\n",
        "    model, history = train_model(model, train_ds, val_ds, base_model)\n",
        "\n",
        "    # Plot results\n",
        "    plot_results(history)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    evaluate_model(model, test_ds)\n",
        "\n",
        "    # Save final model\n",
        "    model.save(\"medicinal_plants_cnn.keras\")\n",
        "    print(\"\\nModel saved as medicinal_plants_cnn.keras\")\n",
        "\n",
        "    # Example prediction\n",
        "    sample_image = next(iter(test_ds))[0][0].numpy()\n",
        "    plt.imshow(sample_image.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Save class names\n",
        "    with open('class_names.txt', 'w') as f:\n",
        "        f.write('\\n'.join(CLASS_NAMES))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}